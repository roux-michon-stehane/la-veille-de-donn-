Notions mathématiques
1°
Un vecteur et une matrice? 
        (Qu’est-ce qu’un vecteur? 
        Un vecteur est un objet généralisant plusieurs notions provenant de la géométrie,

        Qu’est-ce qu’une matrice? 
        Est un élément qui fournit un appui ou une structure, et qui sert à entourer, à reproduire ou à construire.)

2°
Variables indépendantes?
        (Une variable indépendante est un paramètre ou une caractéristique pouvant prendre au moins deux valeurs différentes dont la variation influence la valeur d'une ou de plusieurs autres variables.

        On l'appelle variable indépendante parce qu'elle ne dépend pas du sujet observé. Il existe deux types de variables indépendantes : les variables indépendantes invoquées et les variables indépendantes contrôlées)

3°
Une probabilité?
        (La probabilité d'un événement est un nombre réel. Plus ce nombre est grand, plus le risque, ou la chance, que l'événement se produise est grand.)

4°
Espérance, Variance et Écart-Type?
        (Qu’est-ce qu'une espérance
        Elle correspond à une moyenne pondérée des valeurs que peut prendre cette variable. Dans le cas où celle-ci prend un nombre fini de valeurs, il s'agit d'une moyenne pondérée par les probabilités d'apparition de chaque valeur. Dans le cas où la variable aléatoire possède une densité de probabilité, l'espérance est la moyenne des valeurs pondérées par cette densité. De manière mathématiquement plus précise et plus générale, l'espérance d'une variable aléatoire est l'intégrale de cette variable selon la mesure de probabilité de l'espace probabilisé de départ.

Qu’est-ce qu'une variance
        la variance est une mesure de la dispersion des valeurs d'un échantillon ou d'une distribution de probabilité. Elle exprime la moyenne des carrés des écarts à la moyenne, aussi égale à la différence entre la moyenne des carrés des valeurs de la variable et le carré de la moyenne, selon le théorème de König-Huygens. Ainsi, plus l'écart à la moyenne est grand plus il est prépondérant dans le calcul total (voir la fonction carré) de la variance qui donnerait donc une bonne idée sur la dispersion des valeurs.

        La variance est toujours positive, et ne s’annule que s’il n’y a essentiellement qu’une seule valeur. Sa racine carrée définit l’écart type σ, d’où la notation 
        
        Qu’est-ce qu'un écart type
        Les écarts types sont rencontrés dans tous les domaines où sont appliquées les probabilités et la statistique, en particulier dans le domaine des sondages, en physique, en biologie ou dans la finance. Ils permettent en général de synthétiser les résultats numériques d'une expérience répétée. Tant en probabilités qu'en statistique, il sert à l'expression d'autres notions importantes comme le coefficient de corrélation, le coefficient de variation ou la répartition optimale de Neyman.)

5°
Une corrélation linéaire.
        (Il est égal à 1 dans le cas où l'une des variables est une fonction affine croissante de l'autre variable, à -1 dans le cas où une variable est une fonction affine et décroissante. Les valeurs intermédiaires renseignent sur le degré de dépendance linéaire entre les deux variables. Plus le coefficient est proche des valeurs extrêmes -1 et 1, plus la corrélation linéaire entre les variables est forte ; on emploie simplement l'expression « fortement corrélées » pour qualifier les deux variables. Une corrélation égale à 0 signifie que les variables ne sont pas corrélées linéairement, elles peuvent néanmoins être corrélées non-linéairement, comme on peut le voir sur la troisième ligne de l'image ci-contre.

        Le coefficient de corrélation n’est pas sensible aux unités de chacune des variables. Ainsi, par exemple, le coefficient de corrélation linéaire entre l’âge et le poids d’un individu sera identique que l’âge soit mesuré en semaines, en mois ou en années.

        En revanche, ce coefficient de corrélation est extrêmement sensible à la présence de valeurs aberrantes ou extrêmes (ces valeurs sont appelées des « déviants ») dans notre ensemble de données (valeurs très éloignées de la majorité des autres, pouvant être considérées comme des exceptions).)

6°
Une moyenne, une médiane, un maximum, un ?
        (Qu'es qu'une moyenne?
        La médiane est aussi la valeur centrale qui minimise la valeur moyenne des écarts absolus. Dans la série 

Qu'es qu'une médiane?
        La médiane est aussi la valeur centrale qui minimise la valeur moyenne des écarts absolus. Dans la série {1, 2, 2, 2, 3, 9} donnée auparavant, ce serait (1 + 0 + 0 + 0 + 1 + 7) / 6 = 1,5, plutôt que 1,944 à partir de la moyenne, qui, elle, minimise les écarts quadratiques. En théorie des probabilités, la valeur c qui minimise)

7°
Les quantiles en statistique?
        Les quantiles d'une variable aléatoire univariée, discrète (ex. : entière) ou continue (réelle), sont les valeurs que prend la variable pour des valeurs de probabilité sous le quantile considéré, valant une valeur remarquable, par exemple 3 dixièmes, ou 5 centièmes, etc. On les appelle encore fractiles, synonyme complet selon le contexte d'usage, et ce sont les valeurs réciproques de la fonction de répartition de la loi de probabilité considérée. On s'intéresse plus particulièrement à quelques jeux de valeurs de quantile correspondant aux multiples de fractions simples du 100 % de la probabilité totale. Par exemple, on peut scinder les 100 % de probabilité totale en 4 masses de probabilités égales chacune à 1⁄4=25 %, correspondant, pour les valeurs de la variable aléatoire, à quatre intervalles adjacents. Les trois valeurs intermédiaires définissent ainsi, respectivement les fractiles de 1⁄4=0,25, 1⁄2=0,5 et 3⁄4=0,75, ou encore en termes de fractions, les quantiles d'un quart, un demi et trois quarts (les deux limites extrêmes, l'inférieure correspondant au quantile de 0 et la supérieure pour le quantile de 1, sont les bornes du domaine de définition de la variable aléatoire.

        Les quantiles d'un échantillon statistique de nombres sont des valeurs remarquables permettant de diviser le jeu de ces données ordonnées (i.e. triées) en intervalles consécutifs contenant le même nombre de données (à la justesse de la division entière du nombre total de données, près). Par exemple, un échantillon de 90 données pourra être découpé selon 10 sous-intervalles consécutifs au moyen d'un jeu de 9 quantiles (plus les limites inférieure et supérieure du domaine d'échantillonnage).

8°
Boxplot et Histogramme?
        La boîte à moustaches1 résume seulement quelques indicateurs de position du caractère étudié (médiane, quartiles, minimum, maximum ou déciles). Ce diagramme est utilisé principalement pour comparer un même caractère dans deux populations de tailles différentes.

        Il s'agit de tracer un rectangle allant du premier quartile au troisième quartile et coupé par la médiane. Ce rectangle suffit pour le diagramme en boîte. On ajoute alors des segments aux extrémités menant jusqu'aux valeurs extrêmes, ou jusqu'aux premier et neuvième déciles, voire aux 5e et 95e centiles. On parle alors de diagramme en boîte à moustaches ou de diagramme à pattes.

9°
Fonction de coût?
        (Une fonction de prix peut être une formule mathématique qui permet de déterminer comment les dépenses de production vont évoluer à différents niveaux de production. En d’autres termes, elle estime le coût total de production en fonction d’une quantité sélectionnée produite.

Que signifie la fonction de coût ?

        La direction utilise ce modèle pour exécuter différents scénarios de production et aider à prévoir le coût total de la fourniture d’un produit à différents niveaux de production. L’équation de la fonction de valeur est exprimée par C(x)= FC + V(x), où C est le coût total, FC est le total des coûts fixes, V est le coût variable et x est le nombre d’unités.

        La compréhension de la fonction de coût d’une entreprise est utile dans le cadre du processus budgétaire car elle aide la direction à comprendre le comportement de valeur d’un produit. Cela est souvent vital pour anticiper les coûts qui seront encourus au cours de la prochaine période d’exploitation au niveau de l’activité prévue. Cela permet également à la direction d’évaluer l’efficacité du processus d’assemblage au début de la période d’exploitation.)

10°
Une dérivée?
        (En mathématiques, une fonction permet de définir un résultat (le plus souvent numérique) pour chaque valeur d’un ensemble appelé domaine. Ce résultat peut être obtenu par une suite de calculs arithmétiques ou par une liste de valeurs, notamment dans le cas de relevé de mesures physiques, ou encore par d’autres procédés comme les résolutions d’équations ou les passages à la limite. Le calcul effectif du résultat ou son approximation repose éventuellement sur l’élaboration de fonction informatique.

        Dans l’enseignement scolaire, le terme « fonction » concerne spécifiquement les fonctions réelles d’une variable réelle. De nombreuses fonctions dites usuelles sont ainsi définies comme les fonctions affines, la racine carrée ou l’exponentielle, et peuvent être combinées à l’aide des opérations arithmétiques, de la composition ou de la définition par morceaux.

        Ces fonctions satisfont diverses propriétés portant sur la régularité, les variations, l’intégrabilité...

        En théorie des ensembles, une fonction ou application est une relation entre deux ensembles pour laquelle chaque élément du premier est en relation avec un unique élément du second. Parfois, on distingue la notion de fonction en affaiblissant la condition comme suit : chaque élément du premier ensemble est en relation avec au plus un élément du second.

        En théorie des types, une fonction est la description de la méthode pour obtenir le résultat à partir de ses paramètres. Autrement dit une fonction est l'algorithme qui permet de la calculer.

        Le terme de fonction s'utilise parfois pour des extensions de la notion comme les classes de fonctions p-intégrables ou les distributions telle la fonction de Dirac.)

11°
Descente de gradient.
        (La Descente de Gradient, (ou Gradient Descent en anglais) est un des algorithmes les plus importants de tout le Machine Learning et de tout le Deep Learning. il s’agit d’un algorithme d’optimisation extrêmement puissant qui permet d’entraîner les modèles de régression linéaire, régression logistiques ou encore les réseaux de neurones. Si vous vous lancez dans le Machine Learning, il est donc impératif de comprendre en profondeur l’algorithme de la descente de gradient, et après avoir lu cet article, ça sera chose faite !)

12°
Équation normale?
        (Dans un plan affine euclidien, l'équation d'une droite affine

        Une droite du plan admet exactement deux équations normales, qui correspondent aux deux choix possibles de vecteur normal normé.

        L'avantage de l'équation normale est que si M est un point de coordonnées )

13°
La loi Normale?
        (Les lois de probabilité permettent de décrire de manière théorique le caractère aléatoire d'une expérience qui est considérée comme aléatoire. Les lois normales en sont des cas particuliers. La manière historique de l'aborder est par approximation1.

        Lorsque le résultat de cette expérience aléatoire est à valeurs discrètes, par exemple la somme du lancer de deux dés vaut 2, 3… ou 12, une loi dite discrète modélise l'expérience. Les probabilités d'apparition de chaque valeur peuvent être représentées par des diagrammes en bâtons ou histogrammes (voir la figure ci-contre). Une question que se sont posée plusieurs scientifiques (voir Histoire de la loi normale) est d'effectuer un grand nombre d'expériences et de s'intéresser au comportement de la loi de probabilité associée. Il apparaît que les fréquences d'apparition des valeurs possibles sont de plus en plus « lissées »2 (voir la figure ci-contre). Il existe une certaine répartition autour d'une valeur centrale, ces probabilités peuvent être alors représentées par la courbe de Gauss ou courbe en cloche obtenue par calcul ou par expérience3. Cette courbe est la densité de probabilité d'une loi normale, c'est-à-dire que l'aire sous la courbe vaut 1. Le rôle central de ces lois de probabilité vient du fait qu'elles sont la limite d'un grand nombre de lois de probabilité définies à partir de sommes, comme le montre le théorème central limite.)

14°
Théorème Centrale Limite?
        (Le théorème central limite (aussi appelé théorème limite central, théorème de la limite centrale ou théorème de la limite centrée) établit la convergence en loi de la somme d'une suite de variables aléatoires vers la loi normale. Intuitivement, ce résultat affirme qu'une somme de variables aléatoires indépendantes et identiquement distribuées tend (le plus souvent) vers une variable aléatoire gaussienne.

        Ce théorème et ses généralisations offrent une explication de l'omniprésence de la loi normale dans la nature : de nombreux phénomènes sont dus à l'addition d'un grand nombre de petites perturbations aléatoires.)

15°
Un test d'hypothèse?
        (L'hypothèse nulle notée H0 est celle que l'on considère vraie a priori. L’hypothèse est donc privilégiée et il faut des observations très éloignées de cette hypothèse pour la rejeter. Le but du test est de décider si cette hypothèse est a priori crédible. L'hypothèse alternative notée H1 est l'hypothèse complémentaire à l'hypothèse nulle.

        Ces deux hypothèses ne sont toutefois pas symétriques. H1 est choisie uniquement par défaut si H0 n'est pas considérée comme crédible, {\displaystyle H_{0}}H_0 étant l'hypothèse dont le rejet à tort est le plus préjudiciable. Le choix de H0 et de H1 est en général imposé par le test que l'on utilise et ne relève donc pas de l'utilisateur.)

16°
Chi-Square test?
        (Un test du chi carré est un test d'hypothèse statistique utilisé dans l'analyse des tableaux de contingence lorsque la taille des échantillons est grande. En termes plus simples, ce test est principalement utilisé pour examiner si deux variables catégorielles ( deux dimensions du tableau de contingence ) sont indépendantes pour influencer la statistique de test ( valeurs dans le tableau ).Le test est valide lorsque la statistique de test est distribuée au chi carré sous l' hypothèse nulle , en particulier le test du chi carré de Pearsonet leurs variantes. Le test du chi carré de Pearson est utilisé pour déterminer s'il existe une différence statistiquement significative entre les fréquences attendues et les fréquences observées dans une ou plusieurs catégories d'un tableau de contingence . Pour les tableaux de contingence avec des tailles d'échantillon plus petites, un test exact de Fisher est utilisé à la place.

        Dans les applications standard de ce test, les observations sont classées en classes mutuellement exclusives. Si l' hypothèse nulle selon laquelle il n'y a pas de différences entre les classes de la population est vraie, la statistique de test calculée à partir)

17°
ANOVA?
        (En statistique, l'analyse de la variance (terme souvent abrégé par le terme anglais ANOVA : analysis of variance) est un ensemble de modèles statistiques utilisés pour vérifier si les moyennes des groupes proviennent d'une même population1. Les groupes correspondent aux modalités d'une variable qualitative (p. ex. variable : traitement; modalités : programme d'entrainement sportif, suppléments alimentaires; placebo) et les moyennes sont calculés à partir d'une variable continue (p. ex. gain musculaire).

        Ce test s'applique lorsque l'on mesure une ou plusieurs variables explicatives catégorielle (appelées alors facteurs de variabilité, leurs différentes modalités étant parfois appelées « niveaux ») qui ont de l'influence sur la loi d'une variable continue à expliquer. On parle d'analyse à un facteur lorsque l'analyse porte sur un modèle décrit par un seul facteur de variabilité, d'analyse à deux facteurs ou d'analyse multifactorielle sinon.)

18°
Une valeur propre.
        (Les notions de vecteur propre, de valeur propre, et de sous-espace propre s'appliquent à des endomorphismes (ou opérateurs linéaires), c'est-à-dire des applications linéaires d'un espace vectoriel dans lui-même. Elles sont intimement liées, et forment un pilier de la réduction des endomorphismes, partie de l'algèbre linéaire qui vise à décomposer de la manière la plus efficace possible l'espace en somme directe de sous-espaces stables.)

19°
Analyse en composante principale, Analyse factorielle des correspondances et
Analyse des correspondances multiples.
(

        Analyse en composante principale:
        L'analyse en composantes principales (ACP ou PCA en anglais pour principal component analysis), ou, selon le domaine d'application, transformation de Karhunen–Loève (KLT)1 ou transformation de Hotelling, est une méthode de la famille de l'analyse des données et plus généralement de la statistique multivariée, qui consiste à transformer des variables liées entre elles (dites « corrélées » en statistique) en nouvelles variables décorrélées les unes des autres. Ces nouvelles variables sont nommées « composantes principales » ou axes principaux. Elle permet au statisticien de résumer l'information en réduisant le nombre de variables.

        Il s'agit d'une approche à la fois géométrique2 (les variables étant représentées dans un nouvel espace, selon des directions d'inertie maximale) et statistique (la recherche portant sur des axes indépendants expliquant au mieux la variabilité — la variance — des données). Lorsqu'on veut compresser un ensemble de {\displaystyle N}N variables aléatoires, les {\displaystyle n}n premiers axes de l'analyse en composantes principales sont un meilleur choix, du point de vue de l'inertie ou de la variance.

        L'outil mathématique est appliqué dans d'autres domaines que les statistiques et est parfois appelé décomposition orthogonale aux valeurs propres ou POD (

        Analyse factorielle des correspondances:
        L'analyse factorielle des correspondances (AFC) est une méthode statistique d'analyse des données qui permet d'analyser et de hiérarchiser les informations contenues dans un tableau rectangulaire de données et qui est aujourd'hui particulièrement utilisée pour étudier le lien entre deux variables qualitatives1 (ou catégorielles). Elle a été mise au point à partir des années 1960 par Jean-Paul Benzécri et son équipe, d'abord à la faculté des sciences de Rennes2, puis à celle de Jussieu à Paris au sein du laboratoire de statistique multidimensionnelle. Elle se rattache à la famille des analyses factorielles qui regroupe différentes méthodes d'analyses de grands tableaux rectangulaires de données, visant toutes à identifier et à hiérarchiser des facteurs corrélés aux données placées en colonnes.

        Analyse des correspondances multiples:       
        L’analyse des correspondances multiples (ACM) est une méthode d'analyse factorielle adaptée aux données qualitatives (aussi appelées catégorielles). L'AMC généralise l'analyse factorielle des correspondances (AFC), qui étudie le lien entre deux variables qualitatives, en permettant d'étudier le lien entre plusieurs variables qualitatives1,2. Un exemple typique de ces données est celui des enquêtes d’opinion.

        Discours sur la marche de la classification générale des plantes, depuis Jussieu jusqu'à nos jours (IA jstor-20790308).pdf
        L'ACM permet d'étudier le lien entre ces variables par l'intermédiaire d'un tableau disjonctif complet (TDC) ou du tableau de Burt (TB). Dans de tels tableaux de données, les individus (en lignes) sont décrits par un ensemble de variables qualitatives (en colonnes).)


Outils et librairies

20°
Anaconda, Jupyter-notebook.
(
        Anaconda:
        Anaconda est une distribution libre et open source2 des langages de programmation Python et R appliqué au développement d'applications dédiées à la science des données et à l'apprentissage automatique (traitement de données à grande échelle, analyse prédictive, calcul scientifique), qui vise à simplifier la gestion des paquets et de déploiement3. Les versions de paquetages sont gérées par le système de gestion de paquets conda4. 

        Jupyter-notebook
        Jupyter est une application web utilisée pour programmer dans plus de 40 langages de programmation, dont Python, Julia, Ruby, R, ou encore Scala2. C'est un projet communautaire dont l'objectif est de développer des logiciels libres, des formats ouverts et des services pour l'informatique interactive. Jupyter est une évolution du projet IPython. Jupyter permet de réaliser des calepins ou notebooks, c'est-à-dire des programmes contenant à la fois du texte en markdown et du code. Ces calepins sont utilisés en science des données pour explorer et analyser des données.
)

21°
Librairies Python : Pandas, NumPy, NumPy, Seaborn, Matplotlib, Plotly, Scikit-Learn,
StatsModels, nltk, Pycaret.
(
        Pandas:
        Pandas est une bibliothèque écrite pour le langage de programmation Python permettant la manipulation et l'analyse des données. Elle propose en particulier des structures de données et des opérations de manipulation de tableaux numériques et de séries temporelles.

        Pandas est un logiciel libre sous licence BSD2. Son nom est dérivé du terme Panel Data (en français "données de panel", un terme d'économétrie pour les jeux de données qui comprennent des observations sur plusieurs périodes de temps pour les mêmes individus). Son nom est également un jeu de mots sur l'expression "Python Data Analysis".

        Entre 2007 à 2010, Wes McKinney a commencé à construire ce qui allait devenir Pandas alors qu'il était chercheur dans la société américaine AQR Capital.

        Seaborn:
        Seaborn est une bibliothèque Python de visualisation de données basée sur matplotlib . Il fournit une interface de haut niveau pour dessiner des graphiques statistiques attrayants et informatifs.

        Vous pouvez lire les notes d'introduction ou l' article . Visitez la page d'installation pour voir comment télécharger le package et commencer à l'utiliser. Vous pouvez parcourir la galerie d'exemples pour voir certaines des choses que vous pouvez faire avec seaborn, puis consulter les didacticiels ou la référence de l'API pour savoir comment.

        Pour voir le code ou signaler un bogue, veuillez visiter le référentiel GitHub . Les questions d'assistance générales sont plus à l'aise sur stackoverflow , qui a un canal dédié pour seaborn.

        NumPy:
        NumPy est une bibliothèque pour langage de programmation Python, destinée à manipuler des matrices ou tableaux multidimensionnels ainsi que des fonctions mathématiques opérant sur ces tableaux.

        Plus précisément, cette bibliothèque logicielle libre et open source fournit de multiples fonctions permettant notamment de créer directement un tableau depuis un fichier ou au contraire de sauvegarder un tableau dans un fichier, et manipuler des vecteurs, matrices et polynômes.

        NumPy est la base de SciPy, regroupement de bibliothèques Python autour du calcul scientifique3.

        Matplotlib:
        Matplotlib est une bibliothèque de traçage pour le langage de programmation Python et son extension de mathématiques numériques NumPy . Il fournit une API orientée objet pour intégrer des tracés dans des applications utilisant des kits d' outils GUI à usage général tels que Tkinter , wxPython , Qt ou GTK . Il existe également une interface procédurale "pylab" basée sur une machine à états (comme OpenGL ), conçue pour ressembler étroitement à celle de MATLAB , bien que son utilisation soit déconseillée.  SciPy utilise Matplotlib.

        Matplotlib a été écrit à l'origine par John D. Hunter . Depuis lors, il a eu une communauté de développement active  et est distribué sous une licence de style BSD . Michael Droettboom a été nommé développeur principal de matplotlib peu de temps avant la mort de John Hunter en août 2012  et a ensuite été rejoint par Thomas Caswell. Matplotlib est un projet financé par NumFOCUS . 

        Matplotlib 2.0.x prend en charge les versions Python 2.7 à 3.10. La prise en charge de Python 3 a commencé avec Matplotlib 1.2. Matplotlib 1.4 est la dernière version à prendre en charge Python 2.6.  Matplotlib s'est engagé à ne pas prendre en charge Python 2 après 2020 en signant la déclaration Python 3. 

        Plotly:
        Dash est un framework Python , R et Julia open source pour la création d' applications analytiques basées sur le Web . Il existe de nombreuses bibliothèques Dash open source spécialisées conçues pour créer des composants et des applications Dash spécifiques à un domaine. Quelques exemples sont Dash DAQ, pour la création d'interfaces graphiques d'acquisition de données à utiliser avec des instruments scientifiques, et Dash Bio, qui permet aux utilisateurs de créer des types de graphiques personnalisés, des outils d'analyse de séquence et des outils de rendu 3D pour les applications bioinformatiques. 
        Dash Enterprise est le produit payant de Plotly pour créer, tester, déployer, gérer et faire évoluer les applications Dash à l'échelle de l'organisation. 
        Chart Studio Cloud est un outil en ligne gratuit pour créer des graphiques interactifs. Il dispose d'une interface utilisateur graphique pointer-cliquer pour importer et analyser des données dans une grille et utiliser des outils de statistiques.  Les graphiques peuvent être intégrés ou téléchargés.
        Chart Studio Enterprise est un produit payant qui permet aux équipes de créer, de styliser et de partager des graphiques interactifs sur une seule plateforme. Il offre des options étendues d'authentification et d'exportation de fichiers, et ne limite pas le partage et l'affichage. 
        Bibliothèques de visualisation de données Plotly.js est une bibliothèque JavaScript open source pour créer des graphiques et alimente Plotly.py pour Python , ainsi que Plotly.R pour R , MATLAB , Node.js , Julia et Arduino et une API REST . Plotly peut également être utilisé pour styliser des graphiques interactifs avec Jupyter notebook .
        Convertisseurs de figures qui convertissent les graphiques matplotlib , ggplot2 , et IGOR Pro  en graphiques interactifs en ligne.

        Scikit-Learn:
        Scikit-learn est une bibliothèque libre Python destinée à l'apprentissage automatique. Elle est développée par de nombreux contributeurs2 notamment dans le monde académique par des instituts français d'enseignement supérieur et de recherche comme Inria3.

        Elle propose dans son framework de nombreuses bibliothèques d’algorithmes à implémenter, clé en main. Ces bibliothèques sont à disposition notamment des data scientists.

        Elle comprend notamment des fonctions pour estimer des forêts aléatoires, des régressions logistiques, des algorithmes de classification, et les machines à vecteurs de support. Elle est conçue pour s'harmoniser avec d'autres bibliothèques libres Python, notamment NumPy et SciPy.

        StatsModels:
        Statsmodels est un package Python qui permet aux utilisateurs d'explorer des données, d'estimer des modèles statistiques et d'effectuer des tests statistiques . Une liste complète de statistiques descriptives, de tests statistiques, de fonctions de traçage et de statistiques de résultats est disponible pour différents types de données et chaque estimateur. Il complète le module de statistiques de SciPy .

        Statsmodels fait partie de la pile scientifique Python orientée vers l'analyse de données , la science des données et les statistiques . Statsmodels est construit sur les bibliothèques numériques NumPy et SciPy, s'intègre à Pandas pour la gestion des données et utilise Patsy pour une interface de formule de type R. Les fonctions graphiques sont basées sur la bibliothèque Matplotlib . Statsmodels fournit le backend statistique pour d'autres bibliothèques Python. Statmodels est un logiciel libre publié sous la licence BSD modifiée (3 clauses) .

        nltk:
        Natural Language Toolkit (NLTK) est une bibliothèque logicielle en Python permettant un traitement automatique des langues, développée par Steven Bird et Edward Loper du département d'informatique de l'université de Pennsylvanie. En plus de la bibliothèque, NLTK fournit des démonstrations graphiques, des données-échantillon, des tutoriels, ainsi que la documentation de l'interface de programmation (API).

        Pycaret:
        PyCaret permet à quiconque de créer des solutions d'apprentissage automatique low-code, puissantes et de bout en bout. Il existe des tonnes de façons pour vous de commencer.
)

22°
Librairies R : dplyr, ggplot2, tidyr, tidyverse, Shiny, plotly, Caret.
(
        R:
        R est un langage de programmation et un logiciel libre destiné aux statistiques et à la science des données soutenu par la R Foundation for Statistical Computing. Il fait partie de la liste des paquets GNU3 et est écrit en C, Fortran et R.

        GNU R est un logiciel libre distribué selon les termes de la licence GNU GPL. Le site officiel fournit des binaires pour Linux, Windows et macOS, et des portages existent pour d'autres systèmes d'exploitation.

        Le langage R est largement utilisé par les statisticiens, les data miners, data scientists pour le développement de logiciels statistiques et l'analyse des données.

        En janvier 2022, R est classé 12e dans l'index TIOBE qui mesure la popularité des langages de programmation4.

        dplyr:
        L'un des packages de base du tidyverse dans le langage de programmation R , dplyr est principalement un ensemble de fonctions conçues pour permettre la manipulation de trames de données de manière intuitive et conviviale. Les analystes de données utilisent généralement dplyr afin de transformer des ensembles de données existants dans un format mieux adapté à un type particulier d'analyse ou de visualisation de données.

        Par exemple, quelqu'un qui cherche à analyser un énorme ensemble de données peut souhaiter n'afficher qu'un sous-ensemble plus petit des données. Alternativement, un utilisateur peut souhaiter réorganiser les données afin de voir les lignes classées par une valeur numérique, ou même sur la base d'une combinaison de valeurs de l'ensemble de données d'origine.

        Rédigé principalement par Hadley Wickham , dplyr a été lancé en 2014. [3] Sur la page Web de dplyr, le package est décrit comme "une grammaire de manipulation de données, fournissant un ensemble cohérent de verbes qui vous aident à résoudre les problèmes de manipulation de données les plus courants

        ggplot2:
        ggplot2 est un package de visualisation de données open source pour le langage de programmation statistique R . Créé par Hadley Wickham en 2005, ggplot2 est une implémentation de la grammaire graphique de Leland Wilkinson , un schéma général de visualisation de données qui décompose les graphiques en composants sémantiques tels que des échelles et des couches. ggplot2 peut remplacer les graphiques de base dans R et contient un certain nombre de valeurs par défaut pour l'affichage Web et imprimé des échelles courantes. Depuis 2005, ggplot2 est devenu l'un des packages R les plus populaires. 

        tidyverse:
        Le tidyverse, contraction de tidy et de universe2, est une collection de packages R open source introduite par Hadley Wickham et son équipe. Les packages qui composent le tidyverse partagent "une même philosophie, une même structure et une même structuration des données". Le tidyverse a notamment contribué à encourager et généraliser l'utilisation du piping dans le code.

        Shiny:
        Shiny creates a reactive context wherein the user specifies, through input variables, the circumstances under which computations are re-executed, or graphs (often visualizations) re-rendered; this occurs almost instantaneously. The input variables are evaluated via a user interface which allows the simple creation of widgets such as text boxes, radio buttons, and drop-down lists.

        There are two main parts to a Shiny file, which may alternatively be stored in two separate files. One is designed to accommodate the user interface, the appearance of which is restricted by the default choices, though can be extended through various other R packages. The other is designed to accommodate the server computations and plot generating code, for which all the built-in facilities of R are available.

        Hosting a Shiny app on an RStudio server is free up to certain limits

        plotly:
        (aussi disponible sur python)


        Caret:
        Capacités de CARET 
        Analyse des différences anatomiques de groupe à l'aide de la morphométrie de la profondeur des sillons.
        Affichage des foyers d'activation.
        Génération de surfaces planes, gonflées, sphériques.
        Cartographie des volumes IRMf sur les surfaces.
        Reconstruction de surface à partir de volumes IRM anatomiques à l'aide de l'algorithme SureFit.
        Reconstruction de surface à partir de contours.
        Enregistrement basé sur la surface.
        Visualisation des contours, des surfaces et des volumes.
)

23°
Définition d'un ETL et exemples?
(
        Extract-transform-load est une technologie informatique intergicielle permettant       d'effectuer des synchronisations massives d'information d'une source de données (le plus souvent une base de données) vers une autre. Cette technologie est connue sous le sigle ETL, ou extracto-chargeur. Selon le contexte, il s'agit d'exploiter différentes fonctions, souvent combinées entre elles : « extraction », « transformation », « constitution » ou « conversion », « alimentation » ou « chargement ».
)

24°
Une base de données relationnelle.
(
        En informatique, une base de données relationnelle est une base de données où l'information est organisée dans des tableaux à deux dimensions appelés des relations ou tables1, selon le modèle introduit par Edgar F. Codd en 1960. Selon ce modèle relationnel, une base de données consiste en une ou plusieurs relations. Les lignes de ces relations sont appelées des nuplets ou enregistrements. Les colonnes sont appelées des attributs.

        Les logiciels qui permettent de créer, utiliser et maintenir des bases de données relationnelles sont des systèmes de gestion de bases de données relationnelles (SGBDR).

        Pratiquement tous les systèmes relationnels utilisent le langage SQL pour interroger les bases de données. Ce langage permet de demander des opérations d'algèbre relationnelle telles que l'intersection, la sélection et la jointure.
)

25°
Power BI et Tableau.
(
        Microsoft Power BI est une solution d'analyse de données de Microsoft. Il permet de créer des visualisations de données personnalisées et interactives avec une interface suffisamment simple pour que les utilisateurs finaux créent leurs propres rapports et tableaux de bord2.

        Power BI est un ensemble de services logiciels, d'applications et de connecteurs qui fonctionnent ensemble pour transformer différentes sources de données en informations visuelles, immersives et interactives. Plusieurs sources de données peuvent être utilisées telles que des fichiers Excel, des sources SQL, ou des entrepôts de données hybrides locaux ou sur le cloud. Les données sont personnalisées et interactives avec une interface suffisamment simple pour que les utilisateurs finaux créent leurs propres rapports et tableaux de bord. L'objectif est de faciliter la création des tableaux de bord afin d'améliorer les moyens de communications et de collaboration proposés par Microsoft. Il permet donc de collecter, construire et exposer les données au travers d'indicateurs. Son ergonomie permet par la suite d'animer des présentations interactives qui aideront à la prise de décision.
)

26°
La science des données?
(
        La science des données est l'étude de l’extraction automatisée de connaissance à partir de grands ensembles de données1,2.

        Plus précisément, la science des données est un domaine interdisciplinaire qui utilise des méthodes, des processus, des algorithmes et des systèmes scientifiques pour extraire des connaissances et des idées à partir de nombreuses données structurées ou non . Elle est souvent associée aux données massives et à l'analyse des données.

        Elle utilise des techniques et des théories tirées de nombreux domaines dans le contexte des mathématiques, des statistiques, de l'informatique, de la théorie et des technologies de l'information, parmi lesquelles : l’apprentissage automatique, la compression de données et le calcul à haute performance.

        Histoire

        Cette discipline est issue de l'apparition et du développement des bases de données et de l'Internet et répond aussi à la complexité croissante et au volume en croissance exponentielle du nombre de données numériques disponibles dans le monde (infobésité).

        Elle a reçu beaucoup d'attention dernièrement grâce à l’intérêt grandissant pour les "données massives". Cependant, la science des données ne se limite pas à l’étude de bases de données pouvant être qualifiées de "données massives".

        Par ailleurs, l'essor de techniques d’apprentissage automatique et d’intelligence artificielle a également participé à la croissance de cette discipline et à son ouverture vers de nouveaux champs en passant, par exemple, de l’analyse statistique pure de données fortement structurées à l’analyse de données semi-structurées (XML par exemple) pour notamment mettre « en correspondance des bases de données et de données textuelles

        Compétences métier

        À ne pas confondre avec l’analyse métier et l’ingénierie des données, le scientifique de données va plus loin que l’analyste de données, notamment en utilisant l’apprentissage automatique. Un maître en science des données est quelqu'un qui peut utiliser un certain nombre de méthodes, d'outils et de technologies différents dans le traitement des données pour extraire des observations précieuses à partir de données confuses.

        Pour cela, il doit être rigoureux, mais curieux et créatif, capable de trouver les données les plus adéquates pour une question et avoir une pensée structurée lui permettant de décomposer et organiser les questions et les processus.

        Il doit savoir manipuler et nettoyer les données et les préparer dans un format adapté à l’analyse. Il doit aussi maitriser les sciences des données qui nécessitent une expertise pluridisciplinaire. Son expertise recouvre les domaines scientifiques, méthodologiques, statistiques (maitrise des statistiques descriptives ; moyennes, médianes, variance, déviation, distributions de probabilités, échantillonnage, statistiques inférentielles, etc.), des outils d'ingénierie logicielle du domaine (ex. SAS, R), de l'algorithmique de l'apprentissage automatique, de l'apprentissage profond. Pour déduire des tendances prospectives probables et de bons modèles prédictifs, il peut également s'appuyer sur des bibliothèques (ex. TensorFlow, Keras, PyTorch). Évidemment, ces découvertes s'appuient tant les données du passé et du présent. Il doit maîtriser au moins un langage de programmation (Python, R, Java, Julia, Perl ou C/C++) et un langage de requête de base de données (SQL). Le scientifique de données doit aussi maîtriser les questions de régression et de classification, d'apprentissage supervisé ou non supervisé. Il doit aussi avoir de solides compétences en droit des données et une maitrise des aspects éthiques et sociaux, notamment concernant la confidentialité, l'anonymisation, la sécurité des données sensibles (données personnelles et de santé notamment).

        La maîtrise de la plateforme Hadoop, d'outils de traitement (ex. Hive, Pig...), d'outils d'infonuagique (ex Amazon S3) et la gestion de données non structurées (ex. données issues des réseaux sociaux, de flux vidéo ou audio) est un avantage et peut être requise par certains employeurs. Des notions d'intelligence artificielle sont de plus en plus requises (réseaux de neurones artificiels, etc.). In fine, le scientifique de données doit idéalement aussi être pédagogique, notamment par la maîtrise de la visualisation de données, et il doit être en mesure de déployer les modèles d'apprentissage automatique qu'il a mis au point (c'est-à-dire les rendre utilisables par des non-spécialistes). En rendant les modèles utilisables par les non spécialistes, le scientifique de données crée un « produit de données » (Data product). Celui-ci peut être une application sur un portable ou une application web. Les utilisateurs de R développent généralement leurs produits de données sur Shiny.

        Selon Le Big Data 88 % des scientifiques de données ont au moins une maitrise (master) et 46 % un doctorat. Parmi ces scientifiques de données, 32 % proviennent du domaine des mathématiques et des statistiques, 19 % des sciences informatiques et 16 % d'écoles d’ingénieurs.

        Selon le classement des 25 ou 50 « meilleurs » métiers du monde fait aux États-Unis par le site de recherche d’emploi Glassdoor, celui de Data Scientist arrivait en tête, devant les « ingénieurs DevOps » et les « Data Technicians »
)

27°
L’apprentissage automatique?
(
        L'apprentissage automatique1,2 (en anglais : machine learning, litt. « apprentissage machine1,2 »), apprentissage artificiel1 ou apprentissage statistique est un champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes. On parle d'apprentissage statistique car l'apprentissage consiste à créer un modèle dont l'erreur statistique moyenne est la plus faible possible.

        L'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement réalisée préalablement à l'utilisation pratique du modèle. La seconde phase correspond à la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. En pratique, certains systèmes peuvent poursuivre leur apprentissage une fois en production, pour peu qu'ils aient un moyen d'obtenir un retour sur la qualité des résultats produits.

        Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement3 si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.

        Historique

        Depuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.

        La concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 19364, qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 19505, dans lequel il développe, entre autres, le test de Turing.

        En 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux.

        Arthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis.

        Une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de gagner au jeu Jeopardy. Ce but est atteint en 2011, quand Watson gagne à Jeopardy! en répondant aux questions par traitement de langage naturel.

        Durant les années suivantes, les applications de l'apprentissage automatique médiatisées se succèdent bien plus rapidement qu'auparavant.

        En 2012, un réseau neuronal développé par Google parvient à reconnaître des visages humains ainsi que des chats dans des vidéos YouTube.

        En 2014, 64 ans après la prédiction d'Alan Turing, le dialogueur Eugene Goostman est le premier à réussir le test de Turing en parvenant à convaincre 33 % des juges humains au bout de cinq minutes de conversation qu'il est non pas un ordinateur, mais un garçon ukrainien de 13 ans.

        En 2015, une nouvelle étape importante est atteinte lorsque l'ordinateur « AlphaGo » de Google gagne contre un des meilleurs joueurs au jeu de Go, jeu de plateau considéré comme le plus dur du monde.

        En 2016, un système d'intelligence artificielle à base d'apprentissage automatique nommé LipNet parvient à lire sur les lèvres avec un grand taux de succès

        Principes

        L'apprentissage automatique (AA) permet à un système piloté ou assisté par ordinateur comme un programme, une IA ou un robot, d'adapter ses réponses ou comportements aux situations rencontrées, en se fondant sur l'analyse de données empiriques passées issues de bases de données, de capteurs, ou du web.

        L'AA permet de surmonter la difficulté qui réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire et programmer de manière classique (on parle d'explosion combinatoire). On confie donc à des programmes d'AA le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que les réponses aux données d’entraînement ne sont pas fournies au modèle.

        Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, de forme, d'écriture…), de fouille de données, d'informatique théorique

        Applications

        L'apprentissage automatique est utilisé dans un large spectre d'applications pour doter des ordinateurs ou des machines de capacité d'analyser des données d'entrée comme : perception de leur environnement (vision, Reconnaissance de formes tels des visages, schémas, segmentation d'image, langages naturels, caractères dactylographiés ou manuscrits ; moteurs de recherche, analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, cybersécurité, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; robotique (locomotion de robots, etc.) ; analyse prédictive dans de nombreux domaines (financière, médicale, juridique, judiciaire), diminution des temps de calcul pour les simulations informatiques en physique (calcul de structures, de mécanique des fluides, de neutronique, d'astrophysique, de biologie moléculaire, etc.)18,19, optimisation de design dans l'industrie20,21,22, etc.

        Exemples :

        un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres, mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace
        la reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement identiques. Il existe des systèmes d'apprentissage automatique qui apprennent à reconnaître des caractères en observant des « exemples », c'est-à-dire des caractères connus. Un des premiers système de ce type est celui de reconnaissance des codes postaux US manuscrits issu des travaux de recherche de Yann Le Cun, un des pionniers du domaine 23,24, et ceux utilisés pour la reconnaissance d'écriture ou OCR.
)

28°
L’apprentissage profond.
(
        L’apprentissage profond fait partie d’une famille de méthodes d'apprentissage automatique fondées sur l’apprentissage de modèles de données[Quoi ?]. Une observation (une image, par exemple) peut être représentée de différentes façons par un vecteur, une matrice ou un tenseur de données décrivant la scène observée, notamment en fonction de :

        L’intensité des pixels dont elle est constituée ;
        Ses différentes arêtes[évasif] ;
        Ses différentes régions, aux formes particulières[évasif].
        Certaines[évasif] représentations et une bonne capacité d'analyse automatique des différenciations5[Quoi ?] rendent la tâche d’apprentissage plus efficace[évasif].

        Une des perspectives des techniques de l'apprentissage profond est le remplacement de certaines tâches simples nécessitant la vitesse de traitement d'un ordinateur tels que des calculs[Quoi ?], encore relativement laborieux, par des modèles algorithmiques d’apprentissage supervisé, non supervisé (c’est-à-dire ne prenant pas en compte pas des connaissances spécifiques du domaine étudié) ou encore par des techniques d’extraction hiérarchique[Quoi ?] des caractéristiques.

        Les recherches dans ce domaine s’efforcent de construire de meilleures représentations du réel et de créer des modèles capables d’apprendre ces représentations[pas clair] à partir de données non labellisées[Quoi ?] à grande échelle[Quoi ?]. Certaines[évasif] de ces représentations s’inspirent des dernières avancées en neuroscience. Il s'agit, donc pour résumer d'interprétations du traitement de l'information et des modèles de communication du système nerveux, à l'image de la façon dont le système nerveux établit des connexions en fonction des messages reçus[pas clair], de la réponse neuronale[Quoi ?] et du poids des connexions[Quoi ?] entre les neurones du cerveau.

        Les différentes architectures d’apprentissage profond telles que les réseaux de neurones profonds, les réseaux neuronaux convolutifs « convolutional deep neural networks », et les réseaux de croyance profonde (en) ont plusieurs champs d’application :

                La vision par ordinateur (reconnaissance de formes) ;
                La reconnaissance automatique de la parole ;
                Le traitement automatique du langage naturel ;
                La reconnaissance audio et la bio-informatique6,7.
        Dans ces deux derniers domaines, notamment, elles ont obtenu des résultats très prometteurs.

        Définition

        Une réorganisation et une clarification du contenu sont nécessaires. Améliorez-le ou discutez des points à améliorer.
        Les techniques d'apprentissage profond constituent une classe d’algorithmes d'apprentissage automatique qui :

        Utilisent différentes couches d’unité de traitement non linéaire pour l’extraction et la transformation des caractéristiques ; chaque couche prend en entrée la sortie de la précédente ; les algorithmes peuvent être supervisés ou non supervisés, et leurs applications comprennent la reconnaissance de modèles et la classification statistique ;
        Fonctionnent avec un apprentissage à plusieurs niveaux de détail ou de représentation des données ; à travers les différentes couches, on passe de paramètres de bas niveau à des paramètres de plus haut niveau, où les différents niveaux correspondent à différents niveaux d’abstraction des données.
        Ces architectures permettent aujourd’hui de conférer du « sens » à des données en leur donnant la forme d’images, de sons ou de textes.

        L'apprentissage profond utilise des couches cachées de réseaux de neurones artificiels, des « machines de Boltzmann restreintes », et des séries de calculs propositionnels complexes.

        Les algorithmes d'apprentissage profond s’opposent aux algorithmes d’apprentissage peu profonds du fait du nombre de transformations réalisées sur les données entre la couche d’entrée et la couche de sortie, où une transformation correspond à une unité de traitement définie par des poids et des seuils.
)

29°
Deep vs Machine Learning
(
        L'apprentissage profond1, ou apprentissage en profondeur1 (en anglais : deep learning, deep structured learning, hierarchical learning) est un ensemble de méthodes d'apprentissage automatique tentant de modéliser avec un haut niveau d’abstraction des données grâce à des architectures articulées de différentes transformations non linéaires3. Ces techniques ont permis des progrès importants et rapides dans les domaines de l'analyse du signal sonore ou visuel et notamment de la reconnaissance faciale, de la reconnaissance vocale, de la vision par ordinateur, du traitement automatisé du langage. Dans les années 2000, ces progrès ont suscité des investissements privés, universitaires et publics importants, notamment de la part des GAFAM (Google, Apple, Facebook, Amazon, Microsoft).

        Description et contexte
        L’apprentissage profond fait partie d’une famille de méthodes d'apprentissage automatique fondées sur l’apprentissage de modèles de données[Quoi ?]. Une observation (une image, par exemple) peut être représentée de différentes façons par un vecteur, une matrice ou un tenseur de données décrivant la scène observée, notamment en fonction de :

        L’intensité des pixels dont elle est constituée ;
        Ses différentes arêtes[évasif] ;
        Ses différentes régions, aux formes particulières[évasif].
        Certaines[évasif] représentations et une bonne capacité d'analyse automatique des différenciations[Quoi ?] rendent la tâche d’apprentissage plus efficace[évasif].

        Une des perspectives des techniques de l'apprentissage profond est le remplacement de certaines tâches simples nécessitant la vitesse de traitement d'un ordinateur tels que des calculs[Quoi ?], encore relativement laborieux, par des modèles algorithmiques d’apprentissage supervisé, non supervisé (c’est-à-dire ne prenant pas en compte pas des connaissances spécifiques du domaine étudié) ou encore par des techniques d’extraction hiérarchique[Quoi ?] des caractéristiques.

        Les recherches dans ce domaine s’efforcent de construire de meilleures représentations du réel et de créer des modèles capables d’apprendre ces représentations[pas clair] à partir de données non labellisées[Quoi ?] à grande échelle[Quoi ?]. Certaines[évasif] de ces représentations s’inspirent des dernières avancées en neuroscience. Il s'agit, donc pour résumer d'interprétations du traitement de l'information et des modèles de communication du système nerveux, à l'image de la façon dont le système nerveux établit des connexions en fonction des messages reçus[pas clair], de la réponse neuronale[Quoi ?] et du poids des connexions[Quoi ?] entre les neurones du cerveau.

        Les différentes architectures d’apprentissage profond telles que les réseaux de neurones profonds, les réseaux neuronaux convolutifs « convolutional deep neural networks », et les réseaux de croyance profonde (en) ont plusieurs champs d’application :

        La vision par ordinateur (reconnaissance de formes) ;
        La reconnaissance automatique de la parole ;
        Le traitement automatique du langage naturel ;
        La reconnaissance audio et la bio-informatique.
        Dans ces deux derniers domaines, notamment, elles ont obtenu des résultats très prometteurs.

        Définition
)

30°
La différence entre l'apprentissage supervisé et l'apprentissage non supervisé?
(
        Dans le domaine informatique et de l'intelligence artificielle, l'apprentissage non supervisé désigne la situation d'apprentissage automatique où les données ne sont pas étiquetées. Il s'agit donc de découvrir les structures sous-jacentes à ces données non étiquetées. Puisque les données ne sont pas étiquetées, il est impossible à l'algorithme de calculer de façon certaine un score de réussite.

        L'absence d'étiquetage ou d'annotation caractérise les tâches d'apprentissage non supervisé et les distingue donc des tâches d'apprentissage supervisé.

        L'introduction dans un système d'une approche d'apprentissage non supervisé est un moyen d'expérimenter l'intelligence artificielle. En général, des systèmes d'apprentissage non supervisé permettent d'exécuter des tâches plus complexes que les systèmes d'apprentissage supervisé, mais ils peuvent aussi être plus imprévisibles. Même si un système d'IA d'apprentissage non supervisé parvient tout seul, par exemple, à faire le tri entre des chats et des chiens, il peut aussi ajouter des catégories inattendues et non désirées, et classer des races inhabituelles, introduisant plus de bruit que d'ordre1.

        Apprentissage non supervisé vs. supervisé
        L'apprentissage non supervisé consiste à apprendre sans superviseur. Il s’agit d’extraire des classes ou groupes d’individus présentant des caractéristiques communes. La qualité d'une méthode de classification est mesurée par sa capacité à découvrir certains ou tous les motifs cachés.

        On distingue l'apprentissage supervisé et non supervisé. Dans le premier apprentissage, il s’agit d’apprendre à classer un nouvel individu parmi un ensemble de classes prédéfinies: on connaît les classes a priori. Tandis que dans l'apprentissage non supervisé, le nombre et la définition des classes ne sont pas donnés a priori
)

31°

(
        Dans le domaine informatique et de l'intelligence artificielle, l'apprentissage non supervisé désigne la situation d'apprentissage automatique où les données ne sont pas étiquetées. Il s'agit donc de découvrir les structures sous-jacentes à ces données non étiquetées. Puisque les données ne sont pas étiquetées, il est impossible à l'algorithme de calculer de façon certaine un score de réussite.

        L'absence d'étiquetage ou d'annotation caractérise les tâches d'apprentissage non supervisé et les distingue donc des tâches d'apprentissage supervisé.

        L'introduction dans un système d'une approche d'apprentissage non supervisé est un moyen d'expérimenter l'intelligence artificielle. En général, des systèmes d'apprentissage non supervisé permettent d'exécuter des tâches plus complexes que les systèmes d'apprentissage supervisé, mais ils peuvent aussi être plus imprévisibles. Même si un système d'IA d'apprentissage non supervisé parvient tout seul, par exemple, à faire le tri entre des chats et des chiens, il peut aussi ajouter des catégories inattendues et non désirées, et classer des races inhabituelles, introduisant plus de bruit que d'ordre1.

        Apprentissage non supervisé vs. supervisé
        L'apprentissage non supervisé consiste à apprendre sans superviseur. Il s’agit d’extraire des classes ou groupes d’individus présentant des caractéristiques communes. La qualité d'une méthode de classification est mesurée par sa capacité à découvrir certains ou tous les motifs cachés.

        On distingue l'apprentissage supervisé et non supervisé. Dans le premier apprentissage, il s’agit d’apprendre à classer un nouvel individu parmi un ensemble de classes prédéfinies: on connaît les classes a priori. Tandis que dans l'apprentissage non supervisé, le nombre et la définition des classes ne sont pas donnés a priori 
)

32°
La régression et ses métriques d'évaluation.
(
        En mathématiques, la régression recouvre plusieurs méthodes d’analyse statistique permettant d’approcher une variable à partir d’autres qui lui sont corrélées. Par extension, le terme est aussi utilisé pour certaines méthodes d’ajustement de courbe.

        En apprentissage automatique, on distingue les problèmes de régression des problèmes de classification. Ainsi, on considère que les problèmes de prédiction d'une variable quantitative sont des problèmes de régression tandis que les problèmes de prédiction d'une variable qualitative sont des problèmes de classification. Certaines méthodes, comme la régression logistique, sont à la fois des méthodes de régression au sens où il s'agit de prédire la probabilité d'appartenir à chacune des classes et des méthodes de classification1.

        Histoire
        Le terme provient de la régression vers la moyenne observée par Francis Galton au xixe siècle : les enfants de personnes de grande taille avaient eux-mêmes une taille supérieure à celle de la population en moyenne, mais inférieure à celle de leurs parents (toujours en moyenne), sans que la dispersion de taille au sein de la population totale soit réduite pour autant2,3. Les techniques développées pour quantifier ce phénomène ont engendré des outils de mesure précieux dans tous les champs d’application des statistiques.

        Contexte
        On considère une population d’individus (êtres humains, animaux, pays, biens de consommation…) qui peuvent être décrits selon plusieurs critères appelés variables. Il peut s’agir de variables quantitatives (grandeurs numériques telles que la taille, l’âge, le prix, un pourcentage…) ou qualitatives (sexe, catégorie socio-professionnelle, saison, type de produit…)

        Certaines variables peuvent être plus difficiles à mesurer que d’autres, pour des raisons techniques, des raisons d’accès (données publiques contre données privées), ou encore du fait d’un délai important entre la mise en place d’une expérience et son aboutissement. Il arrive donc que l’on souhaite estimer ces variables (dites expliquées) à partir des données plus faciles à obtenir (dites explicatives). On trouve aussi parfois les appellations variables dépendantes et indépendantes, mais elles présentent des risques de confusion avec la notion d’indépendance en probabilités, or les variables explicatives ne sont pas forcément mutuellement indépendantes.

        La construction de la régression repose d’une part sur une modélisation des variables statistiques par des variables aléatoires (réelles ou non), d’autre part sur un recueil de données croisées, c’est-à-dire que pour un même échantillon de population, on dispose d’observations des différentes variables mesurées avec une imprécision éventuelle.

        La régression consiste alors à formuler un indicateur sur les valeurs de la variable expliquée dépendant uniquement des valeurs des variables explicatives. Cet indicateur pourra ensuite être utilisé sur une population pour laquelle on ne connait que les valeurs des variables explicatives, afin d’estimer les valeurs de la variable expliquée.

        Principe général
        On distingue essentiellement deux cas selon la nature de la variable expliquée, représentée ici par une variable aléatoire Y. Les variables explicatives seront notées X1, … , Xn. Si certaines d’entre elles sont qualitatives, il est parfois judicieux de vectoriser leurs modalités5 en distinguant une modalité de référence représentée par le vecteur nul, et en représentant les autres modalités par les vecteurs de base d’un espace euclidien. Sous certaines conditions, on peut aussi quantifier les modalités de ces variables.

        Cas quantitatif
        Pour une variable expliquée quantitative, c’est-à-dire lorsque Y est une variable aléatoire réelle, la fonction de régression est définie par un indicateur de la loi de Y conditionnellement aux valeurs des autres variables. Il s’agit le plus souvent de l’espérance conditionnelle6, mais on peut aussi considérer d’autres indicateurs de distribution conditionnelle comme la médiane ou d’autres quantiles, le mode, la variance...

        C’est donc une fonction numérique, dont les arguments sont des valeurs possibles des variables explicatives. Il est possible d’étendre cette définition au cas où certaines variables explicatives admettent une fonction de densité8 conjointe avec la variable expliquée.

        Cependant, la définition probabiliste ne suffit pas en général pour décrire la construction de l’indicateur à partir d’un jeu de données statistiques. En effet, les observations ne fournissent pas toujours toutes les combinaisons de modalités dans le cas de variables explicatives discrètes, et ne peuvent être exhaustives dans le cas de variables explicatives continues. La régression s’appuie alors sur un modèle de fonction avec un ou plusieurs paramètres, par exemple une fonction affine dans le cas de la régression linéaire ou multilinéaire. Mais d’autres modèles sont possibles.

        On parle de modèle linéaire lorsque les paramètres apparaissent comme les coefficients d’une combinaison linéaire de fonctions de référence, comme dans le cas de la régression polynomiale ou pour des fonctions puissances ou exponentielles, quitte à passer par un changement de variable9… Mais certains modèles ne sont pas réductibles à un modèle linéaire et leur évaluation peut nécessiter des algorithmes spécifiques, voire recourir à une estimation non paramétrique10.

        Cas qualitatif
        Pour une variable expliquée qualitative, la régression s’apparente aux problèmes de classification, au sens où l’on cherche à déterminer une modalité à partir des valeurs des autres variables.

        La fonction de régression correspond dans ce cas à un partage de l’espace des valeurs des variables explicatives, par des méthodes géométriques ou par régression logistique.

        Qualité de la régression
        La variable expliquée ne s’identifie à la fonction de régression que dans le cas particulier d’une dépendance fonctionnelle. Dans le cas général, on peut interpréter la différence11 comme une erreur aléatoire, souvent notée avec la lettre grecque ε (epsilon).

        Si la fonction de régression est définie par l’espérance conditionnelle, le théorème de l'espérance totale assure alors que l’erreur est centrée. Le théorème de la variance totale donne l’égalité, ce qui permet de montrer que le rapport de corrélation défini par est inférieur à 1, et d’autant plus proche de 1 que la variance de Y conditionnellement à X est faible en moyenne, ce qui en fait un bon indicateur de la qualité de la régression. Inversement, lorsque le rapport de corrélation est proche de 0, cela signifie que la fonction de régression est pratiquement constante, donc que les variables explicatives apportent peu d’information sur l’espérance de la variable expliquée.

        Le cas particulier d’une fonction de régression affine (avec une seule variable X) correspond à l’égalité entre le rapport de corrélation et le coefficient de corrélation linéaire .

        Principaux modèles de régression
        Le modèle de régression le plus connu est le modèle de régression linéaire.

        Lorsque le modèle n'est pas linéaire, on peut effectuer une régression approchée par des algorithmes itératifs, on parle de régression non linéaire.

        Si on s'intéresse au quantile conditionnel de la distribution de la variable aléatoire y sachant le vecteur de variables aléatoires x, on utilise un modèle de régression quantile.

        Si la variable expliquée est une variable aléatoire binomiale, il est courant d'utiliser une régression logistique ou un modèle probit.

        Si la forme fonctionnelle de la régression est inconnue, on peut utiliser un modèle de régression non paramétrique.
)

33°
Le clustering et ses métriques de décision du k optimal et de la qualité des
clusters.
(
        Le clustering est une discipline particulière du Machine Learning ayant pour objectif de séparer vos données en groupes homogènes ayant des caractéristiques communes. C’est un domaine très apprécié en marketing, par exemple, où l’on cherche souvent à segmenter les bases clients pour détecter des comportements particuliers. L’algorithme des K-moyennes (K-means) est un algorithme non supervisé très connu en matière de Clustering. 
        Dans cet article nous allons détailler son fonctionnement et les moyens utiles pour l’optimiser. 

        Cet algorithme a été conçu en 1957 au sein des Laboratoires Bell par Stuart P.Lloyd comme technique de modulation par impulsion et codage(MIC) . Il n’a été présenté au grand publique qu’en 1982. En 1965 Edward W.Forgy avait déjà publié un algorithme quasiment similaire c’est pourquoi le K-means est souvent nommé algorithme de Lloyd-Forgy. 

        Les champs d’application sont divers : segmentation client, analyse de donnée, segmenter une image, apprentissage semi-supervisé….

        Le Principe de l'algorithme des k-means
        Étant donnés des points et un entier k, l’algorithme vise à diviser les points en k groupes, appelés clusters, homogènes et compacts.
)

34°
Traitement automatique du langage?
(
        Le traitement automatique de la langue naturelle (TALN)1 ou traitement automatique des langues (TAL), plus couramment appelé NLP (de l'anglais : natural langage processing) est un domaine multidisciplinaire impliquant la linguistique, l'informatique et l'intelligence artificielle, qui vise à créer des outils de traitement de la langue naturelle pour diverses applications. Il ne doit pas être confondu avec la linguistique informatique, qui vise à comprendre les langues au moyen d'outils informatiques.

        Le TALN est sorti des laboratoires de recherche pour être progressivement mis en œuvre dans des applications informatiques nécessitant l'intégration du langage humain à la machine2. Aussi le TALN est-il parfois appelé ingénierie linguistique. En France, le traitement automatique de la langue naturelle a sa revue, Traitement automatique des langues, publiée par l’Association pour le traitement automatique des langues (ATALA).
)

35°
La réduction de dimensions?
(
        La réduction de la dimensionnalité (ou réduction de (la) dimension) est un processus étudié en mathématiques et en informatique, qui consiste à prendre des données dans un espace de grande dimension, et à les remplacer par des données dans un espace de plus petite dimension. Pour que l'opération soit utile il faut que les données en sortie représentent bien les données d'entrée.

        Définition et buts
        La réduction de dimensionnalité consiste à prendre des données dans un espace de grande dimension, et à les remplacer par des données dans un espace de plus petite dimension.

        La raison pour laquelle une telle opération est utile est que les données de plus petites dimension peuvent être traitées plus rapidement1. Cette opération est cruciale en apprentissage automatique par exemple, pour lutter contre le fléau de la dimension.
)

36°

(
        L'ingénierie de caractéristiques ou l'extraction de caractéristiques ou la découverte de caractéristiques est le processus d'utilisation des connaissances du domaine pour extraire des caractéristiques (caractéristiques, propriétés, attributs) à partir de données brutes. La motivation est d'utiliser ces fonctionnalités supplémentaires pour améliorer la qualité des résultats d'un processus d'apprentissage automatique , par rapport à la fourniture uniquement des données brutes au processus d'apprentissage automatique.

        Processus 
        Le processus d'ingénierie des fonctionnalités est :

        Remue- méninges ou fonctionnalités de test
        Décider des fonctionnalités à créer
        Création de fonctionnalités
        Tester l'impact des fonctionnalités identifiées sur la tâche
        Améliorer vos fonctionnalités si besoin
        Répéter
)

        En apprentissage automatique, une tâche courante est l'étude et la construction d'algorithmes qui peuvent apprendre et faire des prédictions sur les données1. De tels algorithmes fonctionnent en faisant des prédictions ou des décisions basées sur les données2, en construisant un modèle mathématique à partir des données d'entrée. Ces données d'entrée utilisées pour construire le modèle sont généralement divisées en plusieurs jeux de données . En particulier, trois jeux de données sont couramment utilisés à différentes étapes de la création du modèle : les jeux d'apprentissage, de validation et de test.

        Le modèle est initialement ajusté sur un jeu de données d'apprentissage qui est un jeu d'exemples utilisés pour ajuster les paramètres (par exemple, les poids des connexions entre les neurones dans les réseaux de neurones artificiels ) du modèle4. Le modèle (par exemple un classificateur naïf de Bayes ) est entraîné sur le jeu de données d'apprentissage à l'aide d'une méthode d'apprentissage supervisé, par exemple à l'aide de méthodes d'optimisation telles que la descente de gradient ou la descente de gradient stochastique . En pratique, le jeu de données d'apprentissage se compose souvent de paires d'un vecteur d'entrée (ou scalaire) et du vecteur (ou scalaire) de sortie correspondant, où la variable de réponse est communément appelée cible (ou étiquette ou encore tag). Le modèle est exécuté avec le jeu de données d'apprentissage et produit un résultat, qui est ensuite comparé à la cible, pour chaque vecteur d'entrée dans le jeu de données d'apprentissage. Sur la base du résultat de la comparaison et de l'algorithme d'apprentissage spécifique utilisé, les paramètres du modèle sont ajustés. L'ajustement du modèle peut inclure à la fois la sélection de variables et l'estimation des paramètres.

        Successivement, le modèle ajusté est utilisé pour prédire les réponses pour les observations dans un deuxième jeu de données appelé jeu de données de validation. Le jeu de données de validation fournit une évaluation impartiale d'un ajustement de modèle sur le jeu de données d'apprentissage tout en ajustant les hyperparamètres (par exemple, le nombre d'unités cachées couches et largeurs de couche - dans un réseau de neurones). Les jeux de données de validation peuvent être utilisés pour la régularisation par arrêt anticipé (arrêt de l'entraînement lorsque l'erreur sur le jeu de données de validation augmente, car cela est un signe de sur-apprentissage du jeu de données d'entraînement). Cette procédure d'apparence simple est compliquée en pratique par le fait que l'erreur du jeu de données de validation peut fluctuer pendant l'apprentissage, produisant plusieurs minima locaux. Cette complication a conduit à la création de nombreuses règles ad hoc pour décider quand le sur-apprentissage a vraiment commencé.

        Enfin, le jeu de données de test est un jeu de données utilisé pour fournir une évaluation impartiale d'un ajustement final du modèle sur le jeu de données d'apprentissage. Si les données du jeu de données de test n'ont jamais été utilisées dans l'apprentissage (par exemple en validation croisée), le jeu de données de test est également appelé jeu de données d'exclusion. Le terme « jeu de validation » est parfois utilisé au lieu de « jeu de test » dans certaines publications (par exemple, si le jeu de données d'origine a été divisé en deux sous-ensembles seulement, le jeu de test peut être appelé jeu de validation).

        Décider des tailles et des stratégies pour la division des jeux de données dans les jeux d'apprentissage, de test et de validation dépend beaucoup du problème et des données disponibles7.

        Jeu de données d'entraînement
        Un jeu de données d'apprentissage est un ensemble de données d'exemples utilisé pendant le processus d'apprentissage et est utilisé pour ajuster les paramètres (par exemple, les poids) d'un classificateur, par exemple.

        Pour les tâches de classification, un algorithme d'apprentissage supervisé examine le jeu de données d'apprentissage pour déterminer, ou apprendre, les combinaisons optimales de variables qui généreront un bon modèle prédictif. L'objectif est de produire un modèle entraîné (ajusté) qui se généralise bien aux nouvelles données inconnues11. Le modèle ajusté est évalué à l'aide de « nouveaux » exemples issus des jeux de données conservés (jeux de données de validation et de test) pour estimer la précision du modèle dans la classification de nouvelles données. Pour réduire le risque de problèmes tels que le sur-apprentissage, les exemples des jeux de données de validation et de test ne doivent pas être utilisés pour entraîner le modèle.

        La plupart des approches qui recherchent dans les données d'apprentissage des relations empiriques ont tendance à surajuster les données, ce qui signifie qu'elles peuvent identifier et exploiter des relations apparentes dans les données d'apprentissage qui ne sont pas valables en général.

        Jeu de données de validation
        Un jeu de données de validation est un jeu de données d'exemples utilisés pour régler les hyperparamètres (c'est-à-dire l'architecture) d'un classifieur12 Un exemple d'hyperparamètre pour les réseaux de neurones artificiels comprend le nombre d'unités cachées dans chaque couche. Celui-ci, ainsi que le jeu de test (comme mentionné ci-dessus), doit suivre la même Loi de probabilité que le jeu de données d'apprentissage.

        Afin d'éviter le surapprentissage, lorsqu'un paramètre de classification doit être ajusté, il est nécessaire de disposer d'un jeu de données de validation en plus des jeux de données d'apprentissage et de test. Par exemple, si le classificateur le plus approprié pour le problème est recherché, le jeu de données d'apprentissage est utilisé pour entraîner les différents classificateurs candidats, le jeu de données de validation est utilisé pour comparer leurs performances et décider lequel prendre et, enfin, le jeu de test est utilisé pour obtenir les caractéristiques de performance telles que la précision, la sensibilité, la spécificité, la mesure F, etc. Le jeu de données de validation fonctionne comme un hybride : ce sont des données d'entraînement utilisées pour les tests, mais ni dans le cadre de la formation de bas niveau ni dans le cadre du test final.


        Une application de ce processus est en arrêt anticipé, où les modèles candidats sont des itérations successives du même réseau, et la formation s'arrête lorsque l'erreur sur le jeu de validation augmente, en choisissant le modèle précédent (celui avec l'erreur minimale).

        Jeu de données de test
        Un jeu de données de test est un jeu de données indépendant du jeu de données d'apprentissage, mais qui suit la même distribution de probabilité que le jeu de données d'apprentissage. Si un modèle ajusté au jeu de données d'apprentissage s'adapte également bien au jeu de données de test, un surajustement minimal a eu lieu (voir la figure ci-dessous). Un meilleur ajustement du jeu de données d'apprentissage par opposition au jeu de données de test indique généralement un surajustement.

        Un jeu de test est donc un jeu d'exemples utilisés uniquement pour évaluer les performances (c'est-à-dire la généralisation) d'un classificateur entièrement spécifié,. Pour ce faire, le modèle final est utilisé pour prédire les classifications des exemples dans le jeu de test. Ces prédictions sont comparées aux véritables classifications des exemples pour évaluer la précision du modèle10.

        Dans un scénario où les jeux de données de validation et de test sont utilisés, le jeu de données de test est généralement utilisé pour évaluer le modèle final sélectionné au cours du processus de validation. Dans le cas où le jeu de données d'origine est partitionné en deux sous-ensembles (jeu de données d'entraînement et de test), le jeu de données de test peut évaluer le modèle une seule fois (par exemple, dans la méthode d'exclusion ). Notez que certaines sources déconseillent une telle méthode11. Cependant, lors de l'utilisation d'une méthode telle que la validation croisée, deux partitions peuvent être suffisantes et efficaces, car les résultats sont moyennés après des cycles répétés d'entraînement et de test du modèle pour aider à réduire les biais et la variabilité